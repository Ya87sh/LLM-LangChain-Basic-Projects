{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640e4bbc",
   "metadata": {},
   "source": [
    "### Project: Question-Answering on Private Documents (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b748fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Langchain imports\n",
    "from dotenv import load_dotenv, find_dotenv # type: ignore\n",
    "from langchain.document_loaders import PyPDFLoader # type: ignore\n",
    "from langchain.document_loaders import Docx2txtLoader # type: ignore\n",
    "from langchain.document_loaders import TextLoader # type: ignore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # type: ignore\n",
    "from langchain.chains import RetrievalQA # type: ignore\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain_openai import OpenAIEmbeddings # type: ignore\n",
    "from langchain.document_loaders import WikipediaLoader # type: ignore\n",
    "\n",
    "# Importing tiktoken\n",
    "import tiktoken # type: ignore\n",
    "\n",
    "# Importing the necessary libraries for the Pinecone client\n",
    "import pinecone # type: ignore\n",
    "from langchain_community.vectorstores import Pinecone # type: ignore\n",
    "from pinecone import PodSpec # type: ignore\n",
    "\n",
    "# Importing the necessary libraries for the ChromaDB client\n",
    "from langchain.vectorstores import Chroma # type: ignore\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07900179",
   "metadata": {},
   "source": [
    "### Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading PDF, DOCX and TXT files as LangChain Documents\n",
    "def load_document(file):\n",
    "    \n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    \n",
    "    elif extension == '.docx':    \n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    \n",
    "    elif extension == '.txt':\n",
    "        loader = TextLoader(file)\n",
    "    \n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    "\n",
    "    # Load the document\n",
    "    # loader.load(): returns a 'List' of LangChain documents, where each page is a separate Langchain document.\n",
    "    # You can access the content of a particular page using the following command: data[index_number].page_content\n",
    "    # To access metadata of a particular page, use the following command: data[index_number].metadata\n",
    "    data = loader.load()\n",
    "    \n",
    "    # Return the loaded document\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a171597a",
   "metadata": {},
   "source": [
    "### Function to Load data from Wikipedia (Online Services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526fde72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Load Wikipedia\n",
    "# query: is the text used to find content in the Wikipedia\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    \n",
    "    # Creating a Wikipedia Loader object\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    \n",
    "    # Load and Return the data\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bb8ad",
   "metadata": {},
   "source": [
    "### Chunking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c3cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk the data\n",
    "def chunk_data(data, chunk_size=256):\n",
    "    \n",
    "    # Define the Text Splitter Strategy\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    \n",
    "    # Split the data into chunks\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    \n",
    "    # Return the chunks\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b298177",
   "metadata": {},
   "source": [
    "### Calculating Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add191c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f92afa",
   "metadata": {},
   "source": [
    "### Embedding and Uploading to a Vector Database (Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "    \n",
    "    # initializing the Pinecone client and the OpenAI embeddings\n",
    "    pc = pinecone.Pinecone()\n",
    "        \n",
    "    # initializing the OpenAI embeddings\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  # 512 works as well\n",
    "\n",
    "    # loading from existing index\n",
    "    if index_name in pc.list_indexes():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "    \n",
    "    else:\n",
    "        # creating the index and embedding the chunks into the index \n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "\n",
    "        # creating a new index\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=PodSpec(\n",
    "                environment='gcp-starter'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
    "        # inserting the embeddings into the index and returning a new Pinecone vector store object. \n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('Ok')\n",
    "        \n",
    "    return vector_store\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca55774",
   "metadata": {},
   "source": [
    "### Function to delete all the Pinecone Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "\n",
    "    # initializing the Pinecone client\n",
    "    pc = pinecone.Pinecone()\n",
    "    \n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print('Deleting all indexes ... ')\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ...', end='')\n",
    "        pc.delete_index(index_name)\n",
    "        print('Ok')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a94cb",
   "metadata": {},
   "source": [
    "### Function for Asking and Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0a508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, userQues, k=3):\n",
    "    \n",
    "    # initializing the OpenAI language model\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "    # initializing the retriever\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
    "\n",
    "    # initializing the LangChain retrieval QA chain\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    \n",
    "    # Invoking the chain with the question\n",
    "    answer = chain.invoke(userQues)\n",
    "\n",
    "    # returning the answer\n",
    "    return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efe8fc",
   "metadata": {},
   "source": [
    "### Using Chroma as a Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073fa48f",
   "metadata": {},
   "source": [
    "#### Function to create chroma embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aaa44b-fc99-4021-8307-6091e071a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_chroma(chunks, persist_directory='./chroma_db'):\n",
    "\n",
    "    # Instantiate an embedding model from OpenAI (smaller version for efficiency)\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  \n",
    "\n",
    "    # Create a Chroma vector store using the provided text chunks and embedding model, \n",
    "    # configuring it to save data to the specified directory \n",
    "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory) \n",
    "\n",
    "    return vector_store  # Return the created vector store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ae603",
   "metadata": {},
   "source": [
    "#### Function to load chroma embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432555ac-a13f-4e6f-8e5f-e8b350970a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
    "\n",
    "    # Instantiate the same embedding model used during creation\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536) \n",
    "\n",
    "    # Load a Chroma vector store from the specified directory, using the provided embedding function\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings) \n",
    "\n",
    "    return vector_store  # Return the loaded vector store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4938e99-ad31-4093-a5e3-e774fcbd6385",
   "metadata": {},
   "source": [
    "#### Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947584f7-3d74-4562-a4b0-a4588fdc57fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pdf document into LangChain \n",
    "data = load_document('files/rag_powered_by_google_search.pdf')\n",
    "\n",
    "# Splitting the document into chunks\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "\n",
    "# Creating a Chroma vector store using the provided text chunks and embedding model (default is text-embedding-3-small)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625b802-00f7-47ae-b093-5e55cffdbea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asking questions\n",
    "userQuestion = 'What is Vertex AI Search?'\n",
    "answer = ask_and_get_answer(vector_store, userQuestion)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d77f8-aaa7-43b4-83cd-c4087afe718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50633c",
   "metadata": {},
   "source": [
    "#### Code to load an existing Chroma vector store from the specified directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b0c4d-a247-4aa4-b7c4-93c3e656c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a Chroma vector store from the specified directory (default ./chroma_db) \n",
    "db = load_embeddings_chroma()\n",
    "\n",
    "# User Query\n",
    "userQuestion = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "\n",
    "# Get the LLM answer\n",
    "answer = ask_and_get_answer(vector_store, userQuestion)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba55a05-d1c4-4034-b883-967f06a70aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't ask follow-up questions. There is no memory (chat history) available.\n",
    "userQuestion = 'Multiply that number by 2.'\n",
    "answer = ask_and_get_answer(vector_store, userQuestion)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2259f6a4-a2ad-48f2-b414-225b35a49cc5",
   "metadata": {},
   "source": [
    "#### Adding Memory (Chat History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93410211-0cb2-4524-9e90-7d7f67173704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain  # Import class for building conversational AI chains \n",
    "from langchain.memory import ConversationBufferMemory  # Import memory for storing conversation history\n",
    "\n",
    "# Instantiate a ChatGPT LLM (temperature controls randomness)\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)  \n",
    "\n",
    "# Configure vector store to act as a retriever (finding similar items, returning top 5)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})  \n",
    "\n",
    "\n",
    "# Create a memory buffer to track the conversation\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# initializing the LangChain retrieval QA chain with Memory\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,  # Link the ChatGPT LLM\n",
    "    retriever=retriever,  # Link the vector store based retriever\n",
    "    memory=memory,  # Link the conversation memory\n",
    "    chain_type='stuff',  # Specify the chain type\n",
    "    verbose=False  # Set to True to enable verbose logging for debugging\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38309528-51c9-4b4a-b8d4-4b26d3163173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to ask questions\n",
    "def ask_question(q, chain):\n",
    "    result = chain.invoke({'question': q})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0632a8-b463-43e9-91fe-d7d7d8135260",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_document('files/rag_powered_by_google_search.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e143255-faac-4806-abdb-c2b55b82bf95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfda3a7-409c-4511-a698-0165b6acad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb937c3-8769-44af-9afe-fc49c3d5a1f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q = 'Multiply that number by 10.'\n",
    "result = ask_question(q, crc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1848f53-68b5-4265-bf0c-f02b50028fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0120ceea-f701-44de-838c-a4edec180c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q = 'Devide the result by 80.'\n",
    "result = ask_question(q, crc)\n",
    "print(result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f373d4-13fb-4ee9-9c22-c0072a24be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in result['chat_history']:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8c84af-793b-4f40-ba8d-ed57170788e2",
   "metadata": {},
   "source": [
    "### Loop for asking questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd92ad-cc2e-47b0-b8da-112aa8a4c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    q = input('Your question: ')\n",
    "    if q.lower() in 'exit quit bye':\n",
    "        print('Bye bye!')\n",
    "        break\n",
    "    result = ask_question(q, crc)\n",
    "    print(result['answer'])\n",
    "    print('-' * 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808125d7-5026-4771-b7ec-72b8b35c42dc",
   "metadata": {},
   "source": [
    "### Using a Custom Prompt - This is the Main Code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f97e1-eb73-4fb6-b253-13e564dd6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "\n",
    "system_template = r'''\n",
    "Use the following pieces of context to answer the user's question.\n",
    "Before answering translate your response to Spanish.\n",
    "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
    "---------------\n",
    "Context: ```{context}```\n",
    "'''\n",
    "\n",
    "user_template = '''\n",
    "Question: ```{question}```\n",
    "'''\n",
    "\n",
    "messages= [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(user_template)\n",
    "]\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type='stuff',\n",
    "    combine_docs_chain_kwargs={'prompt': qa_prompt},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5667186-1562-4c68-b479-725824f9974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df828fa-8d84-4753-8a0a-dc11d4f1f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing Chroma Vector Store\n",
    "db = load_embeddings_chroma()\n",
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e220f6d5",
   "metadata": {},
   "source": [
    "### Loop for asking questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4822533-ed90-4adb-8c53-13d605b6bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    q = input('Your question: ')\n",
    "    if q.lower() in 'exit quit bye':\n",
    "        print('Bye bye!')\n",
    "        break\n",
    "    result = ask_question(q, crc)\n",
    "    print(result['answer'])\n",
    "    print('-' * 100)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
