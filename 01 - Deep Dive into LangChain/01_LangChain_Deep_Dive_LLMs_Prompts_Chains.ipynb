{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1476ee-4cac-4c75-939e-c6872292a6cc",
   "metadata": {},
   "source": [
    "### Deep Dive into LangChain - LLMs, Prompt Templates, Caching, Streaming, Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee9a618-d423-4cce-8e32-ed937698c0d5",
   "metadata": {},
   "source": [
    "#### Python-dotenv: used to securely retrieve the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e052148c-1dd2-4cd2-8878-fd90e58dabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# loading the .env file\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# Getting the  OpenAI API Keys from the .env file\n",
    "openAPIKey = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "# Getting the ElevenLabs API Key\n",
    "elevenLabsKey = os.environ.get(\"ELEVEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654afaf9-00bc-4d4d-99ab-907055d4139c",
   "metadata": {},
   "source": [
    "### How to Invoke different Chat Models: GPT-3.5 Turbo and GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab38ba77",
   "metadata": {},
   "source": [
    "##### 1. Using ChatOpenAI object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2eb526e-4105-4301-b931-f285f258035b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics is the branch of physics that deals with the behavior of particles at atomic and subatomic scales, where phenomena such as superposition, entanglement, and quantization of energy levels are fundamental.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1, model='gpt-4o', api_key=openAPIKey)\n",
    "\n",
    "# Invoking the LLM model\n",
    "output = llm.invoke('Explain quantum mechanics in one sentence.')\n",
    "\n",
    "# Invoking the GPT-4\n",
    "#output = llm.invoke('Explain quantum mechanics in one sentence.', model='gpt-4', temperature='0.1')\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17211c7b",
   "metadata": {},
   "source": [
    "##### 2. Using ChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2baa8e-8440-4c16-a1bb-91f8ff0ac556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Chat Completions API Messages: System, Assistant and Human\n",
    "from langchain.schema import(SystemMessage, AIMessage, HumanMessage)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a physicist and respond only in German.'),\n",
    "    HumanMessage(content='Explain quantum mechanics in one sentence.')\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e837c-d307-4165-ad99-551a2bee6cf5",
   "metadata": {},
   "source": [
    "### Caching LLM Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379171e-2081-4cc1-a834-89ebff34e84c",
   "metadata": {},
   "source": [
    "##### 1. In-Memory Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73a4e4-d351-4f4c-b149-151fe3b2c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# Setting the in-memory cache\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# Defining the OpenAI object\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')   # It's a slower model, for demo purposes only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4890d235",
   "metadata": {},
   "source": [
    "##### Making the first LLM request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20232db3-97a5-498e-805f-fbcb24d14584",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prompt = 'Tell a me a joke that a toddler can understand.'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d61eb",
   "metadata": {},
   "source": [
    "##### Making the Second LLM request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d58e8e-c8b3-4719-b4ad-a3d03bfb7655",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6269939d-b92f-406a-a53f-9a7bf561c5a2",
   "metadata": {},
   "source": [
    "##### 2. SQLite Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271fe21-1ca3-4cb2-8599-9a368071482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the SQLite cache\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "# Creating a local db for storing cache information\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f4281-b6c7-4a46-ab08-b02528f4111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# First request (not in cache, takes longer)\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe8a61-0812-427b-bdae-06adc67988f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Second request (cached, faster)\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56aa044-5842-4d50-b3e9-0207421df1cf",
   "metadata": {},
   "source": [
    "### LLM Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174adda0-ce4a-40bb-8a28-31abc494f266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Defining ChatOpenAI object\n",
    "llm = ChatOpenAI()\n",
    "prompt = 'Write a rock song about the Moon and a Raven.'\n",
    "\n",
    "# Making the API call\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d50a4-60ca-4009-a051-b83cf70c7a45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's Implement Streaming\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84844cfe-7469-47a6-841c-0fc31d8cdb49",
   "metadata": {},
   "source": [
    "### PromptTemplates - used for tasks that involves 'Text Generation' or 'Completion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34389e61-d08b-440a-8a6f-96e99b5bee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define a template for the prompt\n",
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following virus \"{virus}\" in {language}.'''\n",
    "\n",
    "# Create a PromptTemplate object from the template\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# Fill in the variable: 'virus' and 'language'\n",
    "prompt = prompt_template.format(virus='Covid', language='English')\n",
    "\n",
    "# Returns the generated prompt\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6765f765",
   "metadata": {},
   "source": [
    "##### Making the API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e2f25-4ba8-4a48-b116-4589af56f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "# Native Response\n",
    "#output = llm.invoke(prompt)\n",
    "#print(output.content)\n",
    "\n",
    "# Streaming response\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8084261-b4c6-47e6-8a38-03f339f1f5c5",
   "metadata": {},
   "source": [
    "### ChatPromptTemplates -  used for tasks that involves 'Engaging in Conversation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab3930-930d-4c45-a3d8-a6b213d77270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a chat template with system and human messages\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond only in the JSON format.'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fill in the specific values for n and area\n",
    "messages = chat_template.format_messages(n='5', area='World')\n",
    "\n",
    "# Outputs the formatted chat messages\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a768271-ff8d-4d4c-a7c3-21ebed1abd6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining ChatOpenAI object\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Native Response\n",
    "#output = llm.invoke(messages)\n",
    "#print(output.content)\n",
    "\n",
    "# Streaming Response\n",
    "for chunks in llm.stream(messages):\n",
    "    print(chunks.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2eb160-df97-4229-8115-2e8864b400d3",
   "metadata": {},
   "source": [
    "### Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2aef22-26c3-4e8c-8bbd-99383b6052e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Defining ChatOpenAI model\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Defining Template\n",
    "template = '''You are an experience virologist.\n",
    "Write a few sentences about the following virus \"{virus}\" in {language}.'''\n",
    "\n",
    "# Setting the prompt template\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# Declaring a LLM Chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template, verbose=True)\n",
    "\n",
    "# Getting the Output\n",
    "output = chain.invoke({'virus': 'HSV', 'language': 'Spanish'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f079d-a38d-48f5-8466-9b2c185446f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23cc0a0-6b02-4007-9192-80f00d7576a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = 'What is the capital of {country}?. List the top 3 places to visit in that city. Use bullet points'\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# Initialize an LLMChain with the ChatOpenAI model and the prompt template\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "country = input('Enter Country: ')\n",
    "\n",
    "# Invoke the chain with specific virus and language values\n",
    "output = chain.invoke({'country': country})\n",
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963536b8-76e7-4681-bb0d-8309795e2fa3",
   "metadata": {},
   "source": [
    "### Simple Sequential Chains: First chain's Output is used as Second chain's Input\n",
    "\n",
    "`Each chain has 1 input and 1 output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e711bc-8bcf-45ce-b993-52e42fb650cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# Initialize the first ChatOpenAI model (gpt-3.5-turbo) with specific temperature\n",
    "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "\n",
    "# Define the first prompt template\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template='You are an experienced scientist and Python programmer. Write a function that implements the concept of {concept}.'\n",
    ")\n",
    "\n",
    "# Create an LLMChain using the first model and the prompt template\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "# Initialize the second ChatOpenAI model (gpt-4-turbo) with specific temperature\n",
    "llm2 = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=1.2)\n",
    "\n",
    "# Define the second prompt template\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
    ")\n",
    "# Create another LLMChain using the second model and the prompt template\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)\n",
    "\n",
    "# Combine both chains into a SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "# Invoke the overall chain while specifying only the 'Input Variable' for the 'First Chain' \n",
    "output = overall_chain.invoke('linear regression')\n",
    "\n",
    "# Print the results\n",
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7781c7b",
   "metadata": {},
   "source": [
    "### Run Python code using LangChain: PythonREPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b9413-9d79-47e4-8464-40cb3756700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "# Create PythonREPL agent object\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "# Run the Python code\n",
    "python_repl.run('print([n for n in range(1, 100) if n % 13 == 0])')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea865ea",
   "metadata": {},
   "source": [
    "### LangChain Agents\n",
    "\n",
    "1. The core idea of agents is to use large language models to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order.\n",
    "\n",
    "2. Agents help LLMs to run code, do calculations, search web or run SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9973644c-65a3-44ac-ac9e-1be39950acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent    # Special type of LangChain agent to interact with Python code\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool # Tool to provide Python environment within LangChain allowing LangChain agent to execute py code\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the ChatOpenAI model with gpt-4-turbo and a temperature of 0\n",
    "llm = ChatOpenAI(model='gpt-4-turbo-preview', temperature=0)\n",
    "\n",
    "# Create a Python agent using the ChatOpenAI model and a PythonREPLTool\n",
    "# Tools are functions that allow agents to interact with the outside world\n",
    "agent_executor = create_python_agent(llm=llm, tool=PythonREPLTool(), verbose=True)\n",
    "\n",
    "# Set the Prompt\n",
    "prompt = 'Calculate the square root of the factorial of 12 and display it with 4 decimal points'\n",
    "\n",
    "# Invoke the agent\n",
    "agent_executor.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1046e3-42dd-41a1-be25-8f48454f92f6",
   "metadata": {},
   "source": [
    "### LangChain Tools: DuckDuckGo and Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92f7d0-06c4-475e-9deb-8ab0352050ab",
   "metadata": {},
   "source": [
    "##### 1. DuckDuckGoSearchRun: To Query the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e936c363-5720-443b-b4f9-29d0b5263536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Declare DuckDuckGoSearch Object\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Get more info about DuckDuckGoSearchRun()\n",
    "print(search.description)\n",
    "\n",
    "# Make the call\n",
    "output = search.invoke('Where was Freddie Mercury born?')\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1accfc09-b6c0-4c0a-8cc6-54590d081f86",
   "metadata": {},
   "source": [
    "##### 2. DuckDuckGoSearchResults: To get additional information and links to the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f143f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "\n",
    "# Define Search Result Object\n",
    "search = DuckDuckGoSearchResults()\n",
    "\n",
    "# Invoke the call\n",
    "output = search.run('Freddie Mercury and Queen.')\n",
    "\n",
    "# Print the Output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65118d73",
   "metadata": {},
   "source": [
    "##### 3. DuckDuckGoSearchAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b491d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "# Define Wrapper Object\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(region='in', max_results=3, safesearch='moderate')\n",
    "\n",
    "# Define Search Result Object\n",
    "search = DuckDuckGoSearchResults(api_wrapper=wrapper, source='news')\n",
    "\n",
    "# Invoke the call\n",
    "output = search.run('India')\n",
    "\n",
    "# Print the result\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f49f58",
   "metadata": {},
   "source": [
    "##### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b93a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# Defining Wiki API Wrapper\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=10000)\n",
    "\n",
    "# Wiki QueryRun object\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "\n",
    "# Get the output\n",
    "output = wiki.invoke({'query': 'Google Gemini'})\n",
    "\n",
    "# Print the Output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a860e",
   "metadata": {},
   "source": [
    "### Important: Creating a ReACT Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b518903d",
   "metadata": {},
   "source": [
    "##### What is a ReAct Agent?\n",
    "\n",
    "* ReAct Agent in LangChain: is a system, based on ReAct methodology (Reasoning + Action), that uses LLM model to\n",
    "\n",
    "    * Analyze user's query, and\n",
    "    * Decide which actions to take, in which order, and which tools to use (Python REPL, DuckDuckGo, Wikipedia) to get the info requested in the user query\n",
    "\n",
    "<b>Example:</b>\n",
    "\n",
    "* <b>User Query:</b> The user asks, “what’s the weather in New York?”\n",
    "\n",
    "* <b>Thought:</b> The LLM, thinks about what to do next. It decides that it needs to use a 'Weather Tool; to get the current weather in New York.\n",
    "\n",
    "* <b>Act:</b> The agent uses the weather tool to fetch the weather information. This is like the agent “doing” the action it thought of.\n",
    "\n",
    "* <b>Observe:</b> The agent observes the result from the weather tool. Let’s say the tool returns that it’s currently sunny and 75 degrees Fahrenheit in New York.\n",
    "\n",
    "* <b>Thought:</b> The LLM processes this information and decides that it has enough information to answer the user’s question.\n",
    "\n",
    "* <b>Final Answer:</b> The agent responds to the user, “The current weather in New York is sunny and 75 degrees Fahrenheit.\n",
    "\n",
    "* So, in simple terms, a ReAct agent uses an LLM to 'Think, Act, and Observe' in a loop until it completes a task\n",
    "\n",
    "* The user query guides what task the agent needs to complete, and the available tools help the agent gather the information it needs to complete the task\n",
    "\n",
    "`ReAct Benefits: help overcome Hallucinations and Error Propagations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b2ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# LAngChain Hub: is a place for sharing and discovering resources related to LangChain\n",
    "from langchain import hub\n",
    "\n",
    "from langchain.agents import Tool, AgentExecutor, initialize_agent, create_react_agent\n",
    "\n",
    "# Imports for Tools\n",
    "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_community.tools import ElevenLabsText2SpeechTool\n",
    "\n",
    "# Import for ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the ChatOpenAI model (gpt-4-turbo-preview) with a temperature of 0. Utilize gpt-3.5-turbo if you use the free plan\n",
    "llm = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=0)\n",
    "\n",
    "# Declare a 'User-defined' template for answering questions\n",
    "template = '''\n",
    "Answer the following questions in English as best you can.\n",
    "Questions: {q}\n",
    "'''\n",
    "\n",
    "# Create a PromptTemplate object to use the 'user-defined' template\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# Pull the 'ReAct Prompt Template' from the hub\n",
    "prompt = hub.pull('hwchase17/react')\n",
    "\n",
    "# displaying information about the react prompt\n",
    "# print(type(prompt))\n",
    "# print(prompt.input_variables)\n",
    "# print(prompt.template)\n",
    "\n",
    "\n",
    "# Create tools for the agent\n",
    "\n",
    "# 1. Python REPL Tool (for executing Python code)\n",
    "python_repl = PythonREPLTool()\n",
    "python_repl_tool = Tool(\n",
    "    name='Python REPL',\n",
    "    func=python_repl.run,   #Syntax: <tool>.run\n",
    "    description='Useful when you need to use Python to answer a question. You should input Python code.'\n",
    ")\n",
    "\n",
    "# 2. Wikipedia Tool (for searching Wikipedia)\n",
    "api_wrapper = WikipediaAPIWrapper()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "wikipedia_tool = Tool(\n",
    "    name='Wikipedia',\n",
    "    func=wikipedia.run,\n",
    "    description='Useful for when you need to look up a topic, country, or person on Wikipedia.'\n",
    ")\n",
    "\n",
    "# 3. DuckDuckGo Search Tool (for general web searches)\n",
    "search = DuckDuckGoSearchRun()\n",
    "duckduckgo_tool = Tool(\n",
    "    name='DuckDuckGo Search',\n",
    "    func=search.run,\n",
    "    description='Useful for when you need to perform an internet search to find information that another tool can\\'t provide.'\n",
    ")\n",
    "\n",
    "# 4. ElevenLabs Tool (for Text to Speech)\n",
    "tts = ElevenLabsText2SpeechTool()\n",
    "tts_tool = Tool(\n",
    "    name='ElevenLabs Tool',\n",
    "    func=tts.run,\n",
    "    description=\"Useful for converting Text to Speech\"\n",
    ")\n",
    "\n",
    "# Combine the tools into a list\n",
    "tools = [python_repl_tool, wikipedia_tool, duckduckgo_tool, tts_tool]\n",
    "\n",
    "# Create a 'ReAct Agent' with the ChatOpenAI model, tools, and prompt\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# Initialize the AgentExecutor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1873c6fa",
   "metadata": {},
   "source": [
    "#### 1. Solving a Python Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d41582",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Generate the first 20 numbers in the Fibonacci series.'\n",
    "output = agent_executor.invoke({'input': prompt_template.format(q=question)})\n",
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b7cc66",
   "metadata": {},
   "source": [
    "#### 2. Asking a Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee512373",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Tell me about Napoleon Bonaparte early life'\n",
    "output = agent_executor.invoke({'input': prompt_template.format(q=question)})\n",
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8762f9",
   "metadata": {},
   "source": [
    "#### 3. Text to Speech Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a0aab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Convert the following text into speech- Hello World! The king has arrived\"\n",
    "output = agent_executor.invoke({'input': prompt_template.format(q=question)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d029f6f1",
   "metadata": {},
   "source": [
    "### ElevenLabs Standalone Implementation of 'Text to Speech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbaffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elevenlabs import play\n",
    "from elevenlabs.client import ElevenLabs\n",
    "\n",
    "# Initialize ElevenLabs Object\n",
    "client = ElevenLabs(api_key=elevenLabsKey)\n",
    "\n",
    "# Get all the possible voices\n",
    "outputVoices = client.voices.get_all()\n",
    "\n",
    "audio = client.generate(\n",
    "  text=\"Hi, this is a sample Text to Voice Conversion\",\n",
    "  voice=outputVoices.voices[2],\n",
    "  model=\"eleven_multilingual_v2\"\n",
    ")\n",
    "\n",
    "# To see a list of available voices\n",
    "outputVoices.voices\n",
    "\n",
    "# Play the Audio\n",
    "play(audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
