{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12788d45",
   "metadata": {},
   "source": [
    "### Implementing a ChatGPT App with LangChain from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60530a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Who is the PM of India?', 'text': 'As of now, the Prime Minister of India is Narendra Modi. He has been serving as the Prime Minister since May 2014.'}\n",
      "--------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv # type: ignore\n",
    "load_dotenv(find_dotenv(), override=True) \n",
    "\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.schema import SystemMessage # type: ignore # It sets the behavior of the chatbot.\n",
    "from langchain.chains import LLMChain # type: ignore                                                                                \n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate # type: ignore\n",
    "\n",
    "# Creating ChatOpenAI instance\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "# Creating ChatPromptTemplate instance\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Creating LLMChain instance and starting the conversation\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=False)\n",
    "\n",
    "# Starting the conversation by running the chain\n",
    "while True:\n",
    "    \n",
    "    # Get User Input\n",
    "    userInput = input('Your prompt: ')\n",
    "\n",
    "    # Check if User wants to Quit the conversation\n",
    "    if userInput.lower() in ['quit', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "    \n",
    "    # Invoke the chain\n",
    "    response = chain.invoke({'content': userInput})\n",
    "    print(response)\n",
    "    print('-' * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370c0b7",
   "metadata": {},
   "source": [
    "### Adding Chat Memory Using ConversationBufferMemory\n",
    "\n",
    "Memory is used to store information about past interactions with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a3e2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'earth mass is', 'chat_history': [HumanMessage(content='earth mass is'), AIMessage(content=\"The Earth's mass is approximately 5.97 x 10^24 kilograms. It is the third planet from the Sun in our solar system and is known for its diverse ecosystem and habitability for various forms of life.\")], 'text': \"The Earth's mass is approximately 5.97 x 10^24 kilograms. It is the third planet from the Sun in our solar system and is known for its diverse ecosystem and habitability for various forms of life.\"}\n",
      "--------------------------------------------------\n",
      "{'content': 'its diameter is', 'chat_history': [HumanMessage(content='earth mass is'), AIMessage(content=\"The Earth's mass is approximately 5.97 x 10^24 kilograms. It is the third planet from the Sun in our solar system and is known for its diverse ecosystem and habitability for various forms of life.\"), HumanMessage(content='its diameter is'), AIMessage(content=\"The approximate diameter of Earth is 12,742 kilometers. This measurement represents the distance across the planet passing through its center, which is also known as the Earth's equatorial diameter.\")], 'text': \"The approximate diameter of Earth is 12,742 kilometers. This measurement represents the distance across the planet passing through its center, which is also known as the Earth's equatorial diameter.\"}\n",
      "--------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=False) \n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 1. Imports\n",
    "from langchain.memory import ConversationBufferMemory # It stores all the messages that the Human sends and all the responses that the Chatbot sends.\n",
    "\n",
    "# MessagesPlaceholder: allows us to insert 'List of Historical messages' into the prompt\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder \n",
    "\n",
    "# Create ChatOpenAI instance\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "# 2. Create memory \n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True) # chat_history: is the 'key' where the 'chat memory' will be stored.\n",
    "\n",
    "# 3. Add MessagesPlaceholder(variable_name='messages') to the prompt\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\", \"chat_history\"],\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), # Where the memory will be stored. The placeholder is set before the HumanMessagePromptTemplate\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Add the memory to the chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory, verbose=False)\n",
    "\n",
    "# Starting the conversation by running the chain\n",
    "while True:\n",
    "    \n",
    "    # Get User Input\n",
    "    userInput = input('Your prompt: ')\n",
    "\n",
    "    # Check if User wants to Quit the conversation\n",
    "    if userInput.lower() in ['quit', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "    \n",
    "    # Invoke the chain\n",
    "    response = chain.invoke({'content': userInput})\n",
    "    #print(response)\n",
    "    print(response['text'])\n",
    "    print('-' * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28677aa0",
   "metadata": {},
   "source": [
    "### Saving Chat Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "723b59ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speed of light in a vacuum is approximately 299,792,458 meters per second or about 186,282 miles per second.\n",
      "--------------------------------------------------\n",
      "The speed of light in water is about 225,000 kilometers per second, which is slower than the speed of light in a vacuum. The speed of light in a medium like water is slower due to interactions with atoms and molecules in the material.\n",
      "--------------------------------------------------\n",
      "The speed of light in air is about 299,702 kilometers per second, which is slightly slower than the speed of light in a vacuum but faster than in water. Air has less influence on the speed of light compared to water due to its lower density and fewer interactions with light particles.\n",
      "--------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=False) \n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    " \n",
    "# 1. Import FileChatMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Create ChatOpenAI instance\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "# Creating FileChatMessageHistory instance. All the questions and answers will be stored in 'chat_history.json' file\n",
    "history = FileChatMessageHistory('chat_history.json')\n",
    "\n",
    "# 2. Add an additional keyword argument to the ConversationBufferMemory() constructor\n",
    "# chat_history: is the 'key' where the 'chat memory' will be stored\n",
    "# chat_memory: is the 'history' object that we created above\n",
    "memory = ConversationBufferMemory(memory_key = 'chat_history', chat_memory = history, return_messages=True)\n",
    "\n",
    "# Creating ChatPromptTemplate instance\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\", \"chat_history\"],\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Creating LLMChain instance and starting the conversation\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory, verbose=False)\n",
    "\n",
    "# Starting the conversation by running the chain\n",
    "while True:\n",
    "    \n",
    "    # Get User Input\n",
    "    userInput = input('Your prompt: ')\n",
    "\n",
    "    # Check if User wants to Quit the conversation\n",
    "    if userInput.lower() in ['quit', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "    \n",
    "    # Invoke the chain\n",
    "    response = chain.invoke({'content': userInput})\n",
    "    print(response['text'])\n",
    "    print('-' * 50)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42074641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='light spped in vaccum?'), AIMessage(content='The speed of light in a vacuum is approximately \\\\(299,792,458\\\\) meters per second or about \\\\(186,282\\\\) miles per second.'), HumanMessage(content='light speed in vaccum?'), AIMessage(content='The speed of light in a vacuum is approximately 299,792,458 meters per second or about 186,282 miles per second.'), HumanMessage(content='Light speed in vaccum?'), AIMessage(content='The speed of light in a vacuum is approximately 299,792,458 meters per second or about 186,282 miles per second.'), HumanMessage(content='Light speed in vaccum?'), AIMessage(content='The speed of light in a vacuum is approximately 299,792,458 meters per second or about 186,282 miles per second.'), HumanMessage(content='its speed in water'), AIMessage(content='The speed of light in water is about 225,000 kilometers per second, which is slower than the speed of light in a vacuum. The speed of light in a medium like water is slower due to interactions with atoms and molecules in the material.'), HumanMessage(content='and in air?'), AIMessage(content='The speed of light in air is about 299,702 kilometers per second, which is slightly slower than the speed of light in a vacuum but faster than in water. Air has less influence on the speed of light compared to water due to its lower density and fewer interactions with light particles.')]\n"
     ]
    }
   ],
   "source": [
    "# The messages property contains the list of messages in order.\n",
    "print(history.messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
